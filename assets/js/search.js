
var documents = [{
    "id": 0,
    "url": "https://nipunbatra.github.io/404.html",
    "title": "",
    "body": " 404 Page not found :(  The requested page could not be found. "
    }, {
    "id": 1,
    "url": "https://nipunbatra.github.io/categories/",
    "title": "Tags",
    "body": "Contents: {% if site. categories. size &gt; 0 %} {% for category in site. categories %} {% capture category_name %}{{ category | first }}{% endcapture %} {{ category_name }}{% endfor %}{% endif %} {% for category in site. categories %}  {% capture category_name %}{{ category | first }}{% endcapture %} &lt;h3 id = {{ category_name }} &gt;&lt;i class= fas fa-tags category-tags-icon &gt;&lt;/i&gt;&lt;/i&gt; {{ category_name }}&lt;/h3&gt;&lt;a name= {{ category_name | slugize }} &gt;&lt;/a&gt;{% for post in site. categories[category_name] %}{%- assign date_format = site. minima. date_format | default:  %b %-d, %Y  -%}&lt;article class= archive-item &gt; &lt;p class= post-meta post-meta-title &gt;&lt;a class= page-meta  href= {{ site. baseurl }}{{ post. url }} &gt;{{post. title}}&lt;/a&gt; • {{ post. date | date: date_format }}&lt;/p&gt;&lt;/article&gt;{% endfor %} {% endfor %}"
    }, {
    "id": 2,
    "url": "https://nipunbatra.github.io/images/copied_from_nb/",
    "title": "",
    "body": "WarningDo not manually save images into this folder. This is used by GitHub Actions to automatically copy images.  Any images you save into this folder could be deleted at build time. "
    }, {
    "id": 3,
    "url": "https://nipunbatra.github.io/ml/2020/02/20/bayesian-linear-regression.html",
    "title": "Bayesian Linear Regression",
    "body": "2020/02/20 -                 import numpy as npimport pandas as pdimport matplotlib. pyplot as plt%matplotlib inline          x = np. linspace(-1, 1, 50). reshape(-1, 1)          y = 5*x + 4 noise = (np. abs(x. flatten())*np. random. randn(len(x))). reshape(-1,1)y = y + noise          plt. scatter(x, y)plt. plot(x, 5*x + 4, &#39;k&#39;)  [&lt;matplotlib. lines. Line2D at 0x115c28cd0&gt;]        from scipy. stats import multivariate_normalfrom matplotlib import cmcov = np. array([[ 1 , 0], [0, 1]])var = multivariate_normal(mean=[0,0], cov=cov)x_grid, y_grid = np. mgrid[-1:1:. 01, -1:1:. 01]pos = np. dstack((x_grid, y_grid))z = var. pdf(pos)plt. contourf(x_grid, y_grid, z)plt. gca(). set_aspect(&#39;equal&#39;)plt. xlabel(r&quot;$\theta_0$&quot;)plt. ylabel(r&quot;$\theta_1$&quot;)plt. title(r&quot;Prior distribution of $\theta = f(\mu, \Sigma)$&quot;)plt. colorbar()  &lt;matplotlib. colorbar. Colorbar at 0x1a18423950&gt;  $$\prod_{i=1}^{n} \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(y_{i}-\hat{y}_{i})^{2}}{2 \sigma^{2}}}$$Sample from prior&#182;:       n_samples = 20for n in range(n_samples):  theta_0_s, theta_1_s = var. rvs()  plt. plot(x, theta_1_s*x + theta_0_s, color=&#39;k&#39;,alpha=0. 2)plt. scatter(x, y)  &lt;matplotlib. collections. PathCollection at 0x1a18598fd0&gt;  Likelihood of theta&#182;:       def likelihood(theta_0, theta_1, x, y, sigma):  s = 0  x_plus_1 = np. hstack((np. ones_like(x), x))  for i in range(len(x)):    y_i_hat = x_plus_1[i, :]@np. array([theta_0, theta_1])    s += (y[i,:]-y_i_hat)**2      return np. exp(-s/(2*sigma*sigma))/np. sqrt(2*np. pi*sigma*sigma)          likelihood(-1, 1, x, y, 4)  array([1. 00683395e-22])        x_grid_2, y_grid_2 = np. mgrid[0:8:. 1, 0:8:. 1]li = np. zeros_like(x_grid_2)for i in range(x_grid_2. shape[0]):  for j in range(x_grid_2. shape[1]):    li[i, j] = likelihood(x_grid_2[i, j], y_grid_2[i, j], x, y, 4)              plt. contourf(x_grid_2, y_grid_2, li)plt. gca(). set_aspect(&#39;equal&#39;)plt. xlabel(r&quot;$\theta_0$&quot;)plt. ylabel(r&quot;$\theta_1$&quot;)plt. colorbar()plt. scatter(4, 5, s=200, marker=&#39;*&#39;, color=&#39;r&#39;)plt. title(r&quot;Likelihood as a function of ($\theta_0, \theta_1$)&quot;)  Text(0. 5, 1. 0, &#39;Likelihood as a function of ($\\theta_0, \\theta_1$)&#39;)  Likelihood of $\sigma^2$&#182;:       x_plus_1 = np. hstack((np. ones_like(x), x))theta_mle = np. linalg. inv(x_plus_1. T@x_plus_1)@(x_plus_1. T@y)sigma_2_mle = np. linalg. norm(y - x_plus_1@theta_mle)**2sigma_mle = np. sqrt(sigma_2_mle)sigma_mle  4. 128685902124939  Posterior&#182;: $$\begin{aligned}p(\boldsymbol{\theta} | \mathcal{X}, \mathcal{Y}) &amp;=\mathcal{N}\left(\boldsymbol{\theta} | \boldsymbol{m}_{N}, \boldsymbol{S}_{N}\right) \\\boldsymbol{S}_{N} &amp;=\left(\boldsymbol{S}_{0}^{-1}+\sigma^{-2} \boldsymbol{\Phi}^{\top} \boldsymbol{\Phi}\right)^{-1} \\\boldsymbol{m}_{N} &amp;=\boldsymbol{S}_{N}\left(\boldsymbol{S}_{0}^{-1} \boldsymbol{m}_{0}+\sigma^{-2} \boldsymbol{\Phi}^{\top} \boldsymbol{y}\right)\end{aligned}$$      S0 = np. array([[ 1 , 0], [0, 1]])M0 = np. array([0, 0])SN = np. linalg. inv(np. linalg. inv(S0) + (sigma_mle**-2)*x_plus_1. T@x_plus_1)MN = SN@(np. linalg. inv(S0)@M0 + (sigma_mle**-2)*(x_plus_1. T@y). squeeze())          MN, SN  (array([2. 97803341, 2. 54277597]), array([[2. 54243881e-01, 2. 97285330e-17],    [2. 97285330e-17, 4. 95625685e-01]]))        from scipy. stats import multivariate_normalfrom matplotlib import cmcov = np. array([[ 1 , 0], [0, 1]])var_pos = multivariate_normal(mean=MN, cov=SN)x_grid, y_grid = np. mgrid[0:8:. 1, 0:8:. 1]pos = np. dstack((x_grid, y_grid))z = var_pos. pdf(pos)plt. contourf(x_grid, y_grid, z)plt. gca(). set_aspect(&#39;equal&#39;)plt. xlabel(r&quot;$\theta_0$&quot;)plt. ylabel(r&quot;$\theta_1$&quot;)plt. title(r&quot;Posterior distribution of $\theta = f(\mu, \Sigma)$&quot;)plt. scatter(4, 5, s=200, marker=&#39;*&#39;, color=&#39;r&#39;, label=&#39;MLE&#39;)plt. scatter(MN[0], MN[1], s=100, marker=&#39;^&#39;, color=&#39;black&#39;, label=&#39;MAP&#39;)plt. colorbar()plt. legend()plt. savefig(&quot;. . /images/blr-map. png&quot;)    Sample from posterior       n_samples = 20for n in range(n_samples):  theta_0_s, theta_1_s = var_pos. rvs()  plt. plot(x, theta_1_s*x + theta_0_s, color=&#39;k&#39;,alpha=0. 2)plt. scatter(x, y)  &lt;matplotlib. collections. PathCollection at 0x1a18e7dd10&gt;  Posterior predictions&#182;: $$\begin{aligned}p\left(y_{*} | \mathcal{X}, \mathcal{Y}, \boldsymbol{x}_{*}\right) &amp;=\int p\left(y_{*} | \boldsymbol{x}_{*}, \boldsymbol{\theta}\right) p(\boldsymbol{\theta} | \mathcal{X}, \mathcal{Y}) \mathrm{d} \boldsymbol{\theta} \\&amp;=\int \mathcal{N}\left(y_{*} | \boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{\theta}, \sigma^{2}\right) \mathcal{N}\left(\boldsymbol{\theta} | \boldsymbol{m}_{N}, \boldsymbol{S}_{N}\right) \mathrm{d} \boldsymbol{\theta} \\&amp;=\mathcal{N}\left(y_{*} | \boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{m}_{N}, \boldsymbol{\phi}^{\top}\left(\boldsymbol{x}_{*}\right) \boldsymbol{S}_{N} \boldsymbol{\phi}\left(\boldsymbol{x}_{*}\right)+\sigma^{2}\right)\end{aligned}$$For a point $x*$ Predictive mean = $X^Tm_N$ Predictive variance = $X^TS_NX + \sigma^2$       x_plus_1. T. shape, SN. shape, x_plus_1. shape  ((2, 50), (2, 2), (50, 2))        pred_var = x_plus_1@SN@x_plus_1. Tpred_var. shape  (50, 50)        ## Marginalindividual_var = pred_var. diagonal()          y_hat_map = x_plus_1@MNplt. plot(x, y_hat_map, color=&#39;black&#39;)plt. fill_between(x. flatten(), y_hat_map-individual_var, y_hat_map+individual_var, alpha=0. 2, color=&#39;black&#39;)plt. scatter(x, y)  &lt;matplotlib. collections. PathCollection at 0x1a1881e450&gt;  "
    }, {
    "id": 4,
    "url": "https://nipunbatra.github.io/markdown/2020/01/14/test-markdown.html",
    "title": "Example Markdown Post",
    "body": "2020/01/14 - Basic setup: Jekyll requires blog post files to be named according to the following format: YEAR-MONTH-DAY-filename. md Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. . md is the file extension for markdown files. The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. Basic formatting: You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: Lists: Here’s a list:  item 1 item 2And a numbered list:  item 1 item 2Boxes and stuff:  This is a quotation    You can include alert boxes…and…    You can include info boxesImages: Code: General preformatted text: # Do a thingdo_thing()Python code and output: # Prints '2'print(1+1)2Tables:       Column 1   Column 2         A thing   Another thing   Tweetcards: Altair 4. 0 is released! https://t. co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t. co/roXmzcsT58 . . . read on for some highlights. pic. twitter. com/vWJ0ZveKbZ &mdash; Jake VanderPlas (@jakevdp) December 11, 2019Footnotes:       This is the footnote.  &#8617;    "
    }, {
    "id": 5,
    "url": "https://nipunbatra.github.io/ml/2019/08/20/Gaussian-Processes.html",
    "title": "Gaussian Processes",
    "body": "2019/08/20 -           An example&#182;: Let us look at the GIF above. It shows a non-linear fit with uncertainty on a set of points in the 2d space. The uncertainty is shown by the gray shadowed region. The animation shows how the fit and the uncertainty varies as we keep adding more points (shown as big circles). As expected, as more points are added, the uncertainty of the fit in the vicinity of the added points reduces. This is an example of Gaussian Processes (GP) regression in play. Introduction&#182;: There exist some great online resources for Gaussian Processes (GPs) including an excellent recent Distill. Pub article. This blog post is an attempt with a programatic flavour. In this notebook, we will build the intuition and learn some basics of GPs. This notebook is heavily inspired by the awesome tutorial by Richard Turner. Here is the link to the slides and video. Lectures videos and notes from Nando De Freitas' course are an amazing resource for GPs (and anything ML!). Some imports&#182;:       import numpy as npimport matplotlib. pyplot as pltimport warningswarnings. filterwarnings(&#39;ignore&#39;)%matplotlib inline    A function to make the Matplotlib plots prettier&#182;:       SPINE_COLOR = &#39;gray&#39;def format_axes(ax):  for spine in [&#39;top&#39;, &#39;right&#39;]:    ax. spines[spine]. set_visible(False)  for spine in [&#39;left&#39;, &#39;bottom&#39;]:    ax. spines[spine]. set_color(SPINE_COLOR)    ax. spines[spine]. set_linewidth(0. 5)  ax. xaxis. set_ticks_position(&#39;bottom&#39;)  ax. yaxis. set_ticks_position(&#39;left&#39;)  for axis in [ax. xaxis, ax. yaxis]:    axis. set_tick_params(direction=&#39;out&#39;, color=SPINE_COLOR)  return ax    One dimensional Gaussian/Normal&#182;: We will start the discussion with 1d Gaussians. Let us write some simple code to generate/sample data from $\mathcal{N}(\mu=0, \sigma=1)$       one_dim_normal_data = np. random. normal(0, 1, size=10000)    Let us now visualise the data in a 1d space using scatter plot       plt. scatter(one_dim_normal_data, np. zeros_like(one_dim_normal_data), alpha=0. 2, c=&#39;gray&#39;, edgecolors=&#39;k&#39;, marker=&#39;o&#39;)format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2f0be5b00&gt;  As we would expect, there are a lot of samples close to zero (mean) and as we go further away from zero, the number of samples keeps reducing. We can also visualise the same phenomenon using a normed histogram shown below.       plt. hist(one_dim_normal_data, density=True, bins=20, color=&#39;gray&#39;)format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2f0813588&gt;  We can notice that there is a high probability of drawing samples close to the mean and the probability is low far from the mean. However, since histograms come with their own set of caveats, let us use kernel desnity estimation for obtaining the probability density of 1d Gaussian.       from sklearn. neighbors import KernelDensityx_d = np. linspace(-4, 4, 100)# instantiate and fit the KDE modelkde = KernelDensity(bandwidth=1. 0, kernel=&#39;gaussian&#39;)kde. fit(one_dim_normal_data[:, None])# score_samples returns the log of the probability densitylogprob = kde. score_samples(x_d[:, None])plt. fill_between(x_d, np. exp(logprob), alpha=0. 2, color=&#39;gray&#39;)plt. plot(one_dim_normal_data, np. full_like(one_dim_normal_data, -0. 01), &#39;|k&#39;, markeredgewidth=0. 1)format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2ee1f8c88&gt;  We can now see a smoother version of the histogram and can again verify the properties of 1D Gaussian. Let us now vary the variance of 1D Gaussian and make the same plots to enhance our understanding of the concept.       fig, ax = plt. subplots(ncols=3, sharey=True, figsize=(9, 3))x_d = np. linspace(-6, 6, 400)for i, var in enumerate([0. 5, 1, 2]):  one_dim_normal_data = np. random. normal(0, var, size=10000)  kde = KernelDensity(bandwidth=1. 0, kernel=&#39;gaussian&#39;)  kde. fit(one_dim_normal_data[:, None])  # score_samples returns the log of the probability density  logprob = kde. score_samples(x_d[:, None])  ax[i]. fill_between(x_d, np. exp(logprob), alpha=0. 2, color=&#39;gray&#39;)  ax[i]. plot(one_dim_normal_data, np. full_like(one_dim_normal_data, -0. 01), &#39;|k&#39;, markeredgewidth=0. 1)  format_axes(ax[i])  ax[i]. set_title(f&quot;Variance = {var}&quot;)    We can see that how increasing the variance makes the data more spread. Bi-variate Gaussian&#182;: Having discussed the case of 1d Gaussian, now let us move to multivariate Gaussians. As a special case, let us first consider bi-variate or 2d Gaussian. It's parameters are the mean vector which will have 2 elements and a covariance matrix. We can write the distribution as:$$\begin{pmatrix} X_1 \\ X_2\end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} \mu_1 \\ \mu_2\end{pmatrix} , \begin{pmatrix} a &amp; \rho \\ \rho &amp; b\end{pmatrix} \right)$$ where $\mu_1$, $\mu_2$ are the means for $X_1$ and $X_2$ respectively; $a$ is the standard deviation for $X_1$, $b$ is the standard deviation for $X_2$ and $\rho$ is the correlation between $X_1$ and $X_2$ Let us now draw some data from: $$\begin{pmatrix} X_1 \\ X_2\end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} 0 \\ 0\end{pmatrix} , \begin{pmatrix} 1 &amp; 0. 7 \\ 0. 7 &amp; 1\end{pmatrix} \right)$$       data = np. random. multivariate_normal(mean = np. array([0, 0]), cov = np. array([[1, 0. 7], [0. 7, 1]]), size=(10000, ))          plt. scatter(data[:, 0], data[:, 1], alpha=0. 05,c=&#39;gray&#39;)plt. axhline(0, color=&#39;k&#39;, lw=0. 2)plt. axvline(0, color=&#39;k&#39;, lw=0. 2)plt. xlabel(r&quot;$X_1$&quot;)plt. ylabel(r&quot;$X_2$&quot;)format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2edfeb780&gt;  We can see from the plot above that the data is distributed around mean [0, 0]. We can also see the positive correlation between $X_1$ and $X_2$ Marginalisation for bivariate Gaussian&#182;: Let us look into an interesting plot provided by Seaborn.       import pandas as pddata_df = pd. DataFrame(data, columns=[r&#39;$X_1$&#39;,r&#39;$X_2$&#39;])          import seaborn as snsg = sns. jointplot(x= r&#39;$X_1$&#39;, y=r&#39;$X_2$&#39;, data=data_df, kind=&quot;reg&quot;,color=&#39;gray&#39;)    The central plot is exactly the same as the scatter plot we made earlier. But, we see two additional 1d KDE plots at the top and the right. What do these tell us? These tell us the marginal 1d distributions of $X_1$ and $X_2$. The marginal distribution of $X_1$ is the distribution of $X_1$ considering all values of $X_2$ and vice versa. One of the interesting properties of Gaussian distributions is that the marginal distribution of a Gaussian is also a Gaussian distribution. MathematicalMonk on Youtube has a great set of lectures on this topic that I would highly recommend! What would you expect the marginal distribution of $X_1$ to look like? No prizes for guessing. Given $$\begin{pmatrix} X_1 \\ X_2\end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} \mu_1 \\ \mu_2\end{pmatrix} , \begin{pmatrix} a &amp; \rho \\ \rho &amp; b\end{pmatrix} \right)$$ we have the marginal distribution of:$$X_1 \sim \mathcal{N}(\mu_1, a)$$and $$X_2 \sim \mathcal{N}(\mu_2, b)$$       def plot_jointplot_2d(a, b, rho):  data = np. random. multivariate_normal(mean = np. array([0, 0]), cov = np. array([[a, rho], [rho, b]]), size=(10000, ))  data_df = pd. DataFrame(data, columns=[r&#39;$X_1$&#39;,r&#39;$X_2$&#39;])  g = sns. jointplot(x= r&#39;$X_1$&#39;, y=r&#39;$X_2$&#39;, data=data_df, kind=&quot;reg&quot;,color=&#39;gray&#39;)    Ok, let us know try to plot a few jointplots for different covariance matrices. We would be passing in the values of $a$, $b$ and $\rho$ which would make up the covariance matrix as: \begin{pmatrix} a &amp; \rho \\ \rho &amp; b\end{pmatrix} We would make these plots for mean zero.       plot_jointplot_2d(1, 1, -0. 7)    In the plot above, for $a=1$, $b=1$ and $\rho=0. 7$ we can see the negative correlation (but high) between $X_1$ and $X_2$. Let us now increase the variance in $X_1$ and keep all other paramaters constant.       plot_jointplot_2d(2, 1, -0. 7)    One can see from the plot above that the variance in $X_1$ is much higher now and the plot extends from -6 to +6 for $X_1$ while earlier it was restricted from -4 to 4.       plot_jointplot_2d(1, 1, 0. 0)    One can see from the plot above that the correlation between $X_1$ and $X_2$ is zero. Surface plots for bi-variate Gaussian&#182;: We will now look into surface plots for bi-variate Gaussian. This is yet another way to plot and understand Gaussian distributions. I borrow code from an excellent tuorial on plotting bivariate Gaussians.       from scipy. stats import multivariate_normalfrom mpl_toolkits. mplot3d import Axes3Dfrom matplotlib import cmdef make_pdf_2d_gaussian(mu, sigma):  N = 60  X = np. linspace(-3, 3, N)  Y = np. linspace(-3, 4, N)  X, Y = np. meshgrid(X, Y)  # Pack X and Y into a single 3-dimensional array  pos = np. empty(X. shape + (2,))  pos[:, :, 0] = X  pos[:, :, 1] = Y  F = multivariate_normal(mu, sigma)  Z = F. pdf(pos)  # Create a surface plot and projected filled contour plot under it.   fig = plt. figure()  ax = fig. gca(projection=&#39;3d&#39;)  ax. plot_surface(X, Y, Z, rstride=3, cstride=3, linewidth=1, antialiased=True,          cmap=cm. Greys)    ax. set_xlabel(r&quot;$X_1$&quot;)  ax. set_ylabel(r&quot;$X_2$&quot;)  ax. set_zlabel(&quot;PDF&quot;)  cset = ax. contourf(X, Y, Z, zdir=&#39;z&#39;, offset=-0. 15, cmap=cm. Greys)  # Adjust the limits, ticks and view angle  ax. set_zlim(-0. 15,0. 25)  ax. set_zticks(np. linspace(0,0. 2,5))  ax. view_init(27, -15)  ax. set_title(f&#39;$\mu$ = {mu}\n $\Sigma$ = {sigma}&#39;)          mu = np. array([0. , 0. ])sigma = np. array([[ 1. , -0. 5], [-0. 5, 1]])make_pdf_2d_gaussian(mu, sigma)    From the plot above, we can see the surface plot showing the probability density function for the Gaussian with mean \begin{pmatrix} 0 \\ 0\end{pmatrix} and covariance matrix: \begin{pmatrix} 1 &amp; -0. 5 \\ -0. 5 &amp; 1\end{pmatrix} It can be seen that the probability peaks arounds $X_1=0$ and $X_2=0$. The bottom plot shows the same concept using contour plots which we will heavily use from now on. The different circles in the bottom contour plot denote the loci of same probability density. Since the contour plot requires a lesser dimension, it will be easier to use in our further analysis. Also, from the contour plots, we can see the correlation between $X_1$ and $X_2$.       mu = np. array([0. , 0. ])sigma = np. array([[ 1. , 0], [0, 1]])make_pdf_2d_gaussian(mu, sigma)    In the plot above, we can see that $X_1$ and $X_2$ are not correlated. Contour plots for 2D Gaussians&#182;: Having seen the relationship between the surface plots and the contour plots, we will now exclusively focus on the contour plots. Here is a simple function to generate the contour plot for 2g gaussian with mean and covariance as the arguments.       def plot_2d_contour_pdf(mu, sigma):  X = np. linspace(-3, 3, 60)  Y = np. linspace(-3, 4, 60)  X, Y = np. meshgrid(X, Y)  # Pack X and Y into a single 3-dimensional array  pos = np. empty(X. shape + (2,))  pos[:, :, 0] = X  pos[:, :, 1] = Y  F = multivariate_normal(mu, sigma)  Z = F. pdf(pos)  plt. xlabel(r&quot;$X_1$&quot;)  plt. ylabel(r&quot;$X_2$&quot;)    plt. title(f&#39;$\mu$ = {mu}\n $\Sigma$ = {sigma}&#39;)  plt. contourf(X, Y, Z, zdir=&#39;z&#39;, offset=-0. 15, cmap=cm. Greys)  plt. colorbar()  format_axes(plt. gca())          mu = np. array([0. , 0. ])sigma = np. array([[ 1. , 0. 5], [0. 5, 1. ]])plot_2d_contour_pdf(mu, sigma)  /home/nipunbatra-pc/anaconda3/lib/python3. 7/site-packages/matplotlib/contour. py:1000: UserWarning: The following kwargs were not used by contour: &#39;zdir&#39;, &#39;offset&#39; s)  The plot above shows the contour plot for 2d gaussian with mean [0, 0] and covariance [[ 1. , 0. 5], [0. 5, 1. ]]. We can see the correlation between $X_1$ and $X_2$ Sample from 2d gaussian and visualising it on XY plane&#182;: We will now sample a point from a 2d Gaussian and describe a new way of visualising it.  The left most plot shows the covariance matrix. The middle plot shows the contour plot. The dark point marked in the contour plot is a sampled point (at random) from this 2d Gaussian distribution. The right most plot is an alternative representation of the sampled point. The x-axis corresponds to the labels $X_1$ and $X_2$ and the corresponding y-axis are the coordinates of the point in the $X_1$, $X_2$ dimension shown in the contour plot. We will now write a function to generate a random sample from a 2d gaussian given it's mean and covariance matrix.       def plot_2d_contour_pdf_dimensions(mu, sigma, random_num):  fig, ax = plt. subplots(ncols=3, figsize=(12, 4))  X = np. linspace(-3, 3, 60)  Y = np. linspace(-3, 3, 60)  X, Y = np. meshgrid(X, Y)  # Pack X and Y into a single 3-dimensional array  pos = np. empty(X. shape + (2,))  pos[:, :, 0] = X  pos[:, :, 1] = Y  F = multivariate_normal(mu, sigma)  Z = F. pdf(pos)  random_point = F. rvs(random_state=random_num)    sns. heatmap(sigma, ax=ax[0], annot=True)  ax[1]. contour(X, Y, Z, cmap=cm. Greys)  ax[1]. scatter(random_point[0], random_point[1], color=&#39;k&#39;,s=100)  ax[1]. set_xlabel(r&quot;$X_1$&quot;)  ax[1]. set_ylabel(r&quot;$X_2$&quot;)    data_array = pd. Series(random_point, index=[&#39;X1&#39;,&#39;X2&#39;])  data_array. plot(ax=ax[2], kind=&#39;line&#39;, marker=&#39;o&#39;,color=&#39;k&#39;)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  ax[2]. set_ylim(-3, 3)    format_axes(ax[0])  format_axes(ax[1])  format_axes(ax[2])  ax[0]. set_title(&quot;Covariance Matrix&quot;)  ax[1]. set_title(&quot;Contour of pdf&quot;)  ax[2]. set_title(&quot;Visualising the point&quot;)  plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images&quot;):    os. makedirs(&quot;images&quot;)  if not os. path. exists(f&quot;images/{sigma[0, 1]}&quot;):    os. makedirs(f&quot;images/{sigma[0, 1]}&quot;)  plt. savefig(f&quot;images/{sigma[0, 1]}/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()    We will now create 20 such samples and animate them       for i in range(20):  plot_2d_contour_pdf_dimensions( mu, np. array([[ 1. , 0. 1], [0. 1, 1. ]]), i)          !convert -delay 20 -loop 0 images/0. 1/*. jpg sigma-0-1. gif     Since the correlation between the two variables $X_1$ and $X_2$ was low (0. 1), we can the see that rightmost plot jumping a lot, i. e. to say that the values of $X_1$ and $X_2$ are not tighly constrained to move together.       for i in range(20):  plot_2d_contour_pdf_dimensions( mu, np. array([[ 1. , 0. 7], [0. 7, 1. ]]), i)          !convert -delay 20 -loop 0 images/0. 7/*. jpg sigma-0-7. gif     The above GIF shows the same plot/animation for the 2d Gaussian where the correlation between the two variables is high (0. 7). Thus, we can see that the two variables tend to move up and down together. Conditional Bivariate Distribution&#182;: All excellent till now. Now, let us move to the case in which some variable's values are known. We would then look to find the distribution of the other variables conditional on the value of the known variable. I borrow some text from Wikipedia on the subject. $$\begin{pmatrix} X_1 \\ X_2\end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} 0 \\ 0\end{pmatrix} , \begin{pmatrix} 1 &amp; \rho \\ \rho &amp; 1\end{pmatrix} \right)$$The conditional expectation of $X_2$ given $X_1$ is: $\operatorname{E}(X_2 \mid X_1=x_1)= \rho x_1 $ and the conditional variance is: $\operatorname{var}(X_2 \mid X_1 = x_1) = 1-\rho^2$ So, the question now is: suppose we fix $X_1 = 1$, what is the distribution of $X_2$. Again, Gaussians are amazing - the conditional distributionon is again a Gaussian. Let us make some plots to understand better. The following plots would be showing the distribution of $X_2$ with fixed $X_1$       def plot_2d_contour_pdf_dimensions_fixed_x1(sigma, random_num, x1 = 1):  mu = np. zeros(2)  fig, ax = plt. subplots(ncols=3, figsize=(12, 4))  X = np. linspace(-3, 3, 60)  Y = np. linspace(-3, 3, 60)  X, Y = np. meshgrid(X, Y)  # Pack X and Y into a single 3-dimensional array  pos = np. empty(X. shape + (2,))  pos[:, :, 0] = X  pos[:, :, 1] = Y  F = multivariate_normal(mu, sigma)  Z = F. pdf(pos)    rho = sigma[0, 1]  F_cond_x1 = multivariate_normal(rho*x1, 1-rho**2)  random_point_x2 = F_cond_x1. rvs(random_state=random_num)  sns. heatmap(sigma, ax=ax[0], annot=True)  ax[1]. contour(X, Y, Z, cmap=cm. Greys)  ax[1]. scatter(x1, random_point_x2, color=&#39;k&#39;,s=100)  ax[1]. set_xlabel(r&quot;$X_1$&quot;)  ax[1]. set_ylabel(r&quot;$X_2$&quot;)    data_array = pd. Series([x1, random_point_x2], index=[&#39;X1&#39;,&#39;X2&#39;])  data_array. plot(ax=ax[2], kind=&#39;line&#39;, color=&#39;k&#39;)  ax[2]. scatter(x=0, y=x1, color=&#39;red&#39;, s=100)  ax[2]. scatter(x=1, y=random_point_x2, color=&#39;k&#39;, s=100)    plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  ax[2]. set_ylim(-3, 3)  format_axes(ax[0])  format_axes(ax[1])  format_axes(ax[2])  ax[0]. set_title(&quot;Covariance Matrix&quot;)  ax[1]. set_title(&quot;Contour of pdf&quot;)  ax[2]. set_title(&quot;Visualising the point&quot;)  plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images/conditional/&quot;):    os. makedirs(&quot;images/conditional/&quot;)  if not os. path. exists(f&quot;images/conditional/{sigma[0, 1]}&quot;):    os. makedirs(f&quot;images/conditional/{sigma[0, 1]}&quot;)  plt. savefig(f&quot;images/conditional/{sigma[0, 1]}/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()          for i in range(20):  plot_2d_contour_pdf_dimensions_fixed_x1(np. array([[ 1. , 0. 1], [0. 1, 1. ]]), i)          !convert -delay 20 -loop 0 images/conditional/0. 1/*. jpg conditional-sigma-0-1. gif     The above animation shows the movement of $X_2$ with $X_1=1$. The $X_1=1$ is shown in red in the righmost plot. In the middle plot, we can confirm that the movement is only in the $X_2$ dimension. Further, since the correlation between $X_1$ and $X_2$ is weak, the righmost plot seems to wiggle or jump a lot!       for i in range(20):  plot_2d_contour_pdf_dimensions_fixed_x1(np. array([[ 1. , 0. 7], [0. 7, 1. ]]), i)          !convert -delay 20 -loop 0 images/conditional/0. 7/*. jpg conditional-sigma-0-7. gif     In the plot above, we repeat the same p|rocedure but with a covariance matrix having a much higher correlation between $X_1$ and $X_2$. From the righmost plot, we can clearly see that the jumps in $X2$ are far lesser. This is expected, since the two variables are correlated! Visualising the same procedure for 5 dimensional Gaussian&#182;: We will now repeat the same procedure we did for 2d case in 5 dimensions.       covariance_5d = np. array([[1, 0. 9, 0. 8, 0. 6, 0. 4],             [0. 9, 1, 0. 9, 0. 8, 0. 6],             [0. 8, 0. 9, 1, 0. 9, 0. 8],             [0. 6, 0. 8, 0. 9, 1, 0. 9],             [0. 4, 0. 6, 0. 8, 0. 9, 1]])          def plot_5d_contour_pdf_dimensions(cov, random_num):  fig, ax = plt. subplots(ncols=2, figsize=(6, 3))  mu = np. zeros(5)  F = multivariate_normal(mu, cov)  random_point = F. rvs(random_state=random_num)    sns. heatmap(cov, ax=ax[0], annot=True)      data_array = pd. Series(random_point, index=[&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;X4&#39;, &#39;X5&#39;])  data_array. plot(ax=ax[1], kind=&#39;line&#39;, marker=&#39;o&#39;,color=&#39;k&#39;)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  ax[1]. set_ylim(-3, 3)  for i in range(2):    format_axes(ax[i])    ax[0]. set_title(&quot;Covariance Matrix&quot;)  ax[-1]. set_title(&quot;Visualising the point&quot;)  plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images/5d/&quot;):    os. makedirs(&quot;images/5d&quot;)    plt. savefig(f&quot;images/5d/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()          plot_5d_contour_pdf_dimensions(covariance_5d, 2)          for i in range(20):  plot_5d_contour_pdf_dimensions(covariance_5d, i)          !convert -delay 20 -loop 0 images/5d/*. jpg 5d. gif     From the visualisation above we can see that: since X1 and X2 are highly correlated, they move up and down togetherbut, X1 and X5 have low correlation, thus, they can seem to wiggle almost independently of each other. We are now getting somewhere. If the correlation between the variables is very high, we will get a smooth curve joining them. Right? Almost getting to the point where we can draw the introductory plot shown at the top of the post. Conditional Multivariate Distribution&#182;: Ok, now let us draw the conditional distribution over this higher 5d space. We will fix the values of some of the variables and see the distribution of the others. Borrowing from Wikipedia If $N$-dimensional $x$ is partitioned as follows $$\mathbf{x}=\begin{bmatrix} \mathbf{x}_A \\ \mathbf{x}_B\end{bmatrix}\text{ with sizes }\begin{bmatrix} q \times 1 \\ (N-q) \times 1 \end{bmatrix}$$and accordingly $μ$ and $Σ$ are partitioned as follows $$\boldsymbol\mu=\begin{bmatrix} \boldsymbol\mu_A \\ \boldsymbol\mu_B\end{bmatrix}\text{ with sizes }\begin{bmatrix} q \times 1 \\ (N-q) \times 1 \end{bmatrix}$$$$\boldsymbol\Sigma=\begin{bmatrix} \boldsymbol\Sigma_{AA} &amp; \boldsymbol\Sigma_{AB} \\ \boldsymbol\Sigma_{BA} &amp; \boldsymbol\Sigma_{BB}\end{bmatrix}\text{ with sizes }\begin{bmatrix} q \times q &amp; q \times (N-q) \\ (N-q) \times q &amp; (N-q) \times (N-q) \end{bmatrix}$$then the distribution of $x_A$ conditional on $x_B=b$ is multivariate normal $(x_A|x_B=b)\sim \mathcal{N}(\bar{\mu}, \bar{\Sigma})$ $$\bar{\boldsymbol\mu}=\boldsymbol\mu_A + \boldsymbol\Sigma_{AB} \boldsymbol\Sigma_{BB}^{-1}\left( \mathbf{B} - \boldsymbol\mu_B\right)$$and covariance matrix $$\overline{\boldsymbol\Sigma}=\boldsymbol\Sigma_{AA} - \boldsymbol\Sigma_{AB} \boldsymbol\Sigma_{BB}^{-1} \boldsymbol\Sigma_{BA}. $$Let us for our example take $X_5 = -2$. We have: $x_A = [x_1, x_2, x_3, x_4]$ and $x_B = [x_5]$ Assuming the covariance matrix of size 5 X 5 is referred as $C$ $$\boldsymbol\Sigma_{AA}=\begin{bmatrix} C_{11} &amp; C_{12} &amp; C_{13} &amp; C_{14}\\ C_{21} &amp; C_{22} &amp; C_{23} &amp; C_{24}\\ C_{31} &amp; C_{32} &amp; C_{33} &amp; C_{34}\\ C_{41} &amp; C_{42} &amp; C_{43} &amp; C_{44}\\\end{bmatrix} \\$$$$\boldsymbol\Sigma_{AB}=\begin{bmatrix} C_{15}\\ C_{25}\\ C_{35}\\ C_{45}\\\end{bmatrix}$$$$\boldsymbol\Sigma_{BA}=\begin{bmatrix} C_{51}&amp; C_{52} &amp; C_{53} &amp; C_{54}\\\end{bmatrix}$$$$\boldsymbol\Sigma_{BB}=\begin{bmatrix} C_{55}\\\end{bmatrix}$$Putting in the numbers we get:       sigma_AA = covariance_5d[:4, :4]          sigma_AA  array([[1. , 0. 9, 0. 8, 0. 6],    [0. 9, 1. , 0. 9, 0. 8],    [0. 8, 0. 9, 1. , 0. 9],    [0. 6, 0. 8, 0. 9, 1. ]])        sigma_AB = covariance_5d[:4, 4]. reshape(-1, 1)          sigma_AB  array([[0. 4],    [0. 6],    [0. 8],    [0. 9]])        sigma_BA = covariance_5d[4, :4]. reshape(1, -1)          sigma_BA  array([[0. 4, 0. 6, 0. 8, 0. 9]])        sigma_BB = covariance_5d[4, 4]. reshape(-1, 1)          sigma_BB  array([[1. ]])  Now, calculating $\bar{\mu}$       mu_bar = np. zeros((4, 1)) + sigma_AB@np. linalg. inv(sigma_BB)*(-2)          mu_bar  array([[-0. 8],    [-1. 2],    [-1. 6],    [-1. 8]])  Since, $x_5$ has highest correlation with $x_4$ it makes sense for $x_5=-2$ to have the mean of $x_4$ to be close to -2. Now, calculating $\bar{\Sigma}$       sigma_bar = sigma_AA - sigma_AB@np. linalg. inv(sigma_BB)@sigma_BA          sigma_bar  array([[0. 84, 0. 66, 0. 48, 0. 24],    [0. 66, 0. 64, 0. 42, 0. 26],    [0. 48, 0. 42, 0. 36, 0. 18],    [0. 24, 0. 26, 0. 18, 0. 19]])  Now, we have the new mean and covariance matrices for $x_A = [x_1, x_2, x_3, x_4]$ and $x_B = [x_5] = [-2]$. Let us now draw some samples fixing $x_5 = -2$       cov = sigma_barmu = mu_bar. flatten()def plot_5d_samples_fixed_x2(random_num):  fig, ax = plt. subplots(ncols=2, figsize=(6, 3))      F = multivariate_normal(mu, cov)    sns. heatmap(cov, ax=ax[0], annot=True)  random_point = F. rvs(random_state=random_num)      data_array = pd. Series(random_point, index=[&#39;X1&#39;,&#39;X2&#39;,&#39;X3&#39;,&#39;X4&#39;])  data_array[&#39;X5&#39;] = -2  data_array. plot(ax=ax[1], kind=&#39;line&#39;, marker=&#39;. &#39;,color=&#39;k&#39;)  plt. scatter([4], [-2], color=&#39;red&#39;, s=100)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  ax[1]. set_ylim(-3, 3)  for i in range(2):    format_axes(ax[i])    ax[0]. set_title(&quot;Covariance Matrix&quot;)  ax[-1]. set_title(&quot;Visualising the point&quot;)  plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images/5d/conditional/1&quot;):    os. makedirs(&quot;images/5d/conditional/1&quot;)    plt. savefig(f&quot;images/5d/conditional/1/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()            for i in range(20):  plot_5d_samples_fixed_x2(i)          !convert -delay 20 -loop 0 images/5d/conditional/1/*. jpg 5d-conditional-1. gif     Let's increase to 20 dimensions now!&#182;: We can not surely write the covariance matrix for 20 dimensions. Let us use a small trick called the kernel function to create this matrix. We will come it later. For now, let us think of this function as a function which: outputs low numbers for $x_1$ and $x_2$ if they differ by a lotoutputs high number for $x_1$ and $x_2$ if they are very close      def rbf_kernel(x_1, x_2, sig):  return np. exp((-(x_1-x_2)**2)/2*(sig**2))          rbf_kernel(1, 1, 0. 4)  1. 0  Since 1=1, the above function evaluates to 1 showing that 1 is similar to 1       rbf_kernel(1, 2, 0. 4)  0. 9231163463866358  Since 1 and 2 are close, the function above evaluates to close to 1       rbf_kernel(1, 2, 1)  0. 6065306597126334  Ok, we use the same first two arguments 1 and 2 but change the last one to 1 from 0. 4 and we see that the function evaluates to a much smaller number. Thus, we can see that increase the sig parameter leads to quicker dropoff in similarity between pair of points. Or, in other words, higher sig means that the influence of a point x_1 reduces quicker. Let us now create the covariance matrix of size (20, 20) using this kernel function.       C = np. zeros((20, 20))          for i in range(20):  for j in range(20):    C[i, j] = rbf_kernel(i, j, 0. 5)    Let us plot the heatmap of the covariance matrix       sns. heatmap(C)  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d3922fd0&gt;  The above heatmap confirms that there is correlation between nearby points, but close to zero or zero correlation otherwise. Let us draw some samples from this 20 dimensional Gaussian&#182;:       def plot_20d_samples(random_num):  fig, ax = plt. subplots(figsize=(10, 3))      F = multivariate_normal(np. zeros(20), C)  random_point = F. rvs(random_state=random_num)  index = [f&#39;X{i}&#39; for i in range(1, 21)]  data_array = pd. Series(random_point, index=index)  data_array. plot(ax=ax, kind=&#39;line&#39;, marker=&#39;. &#39;,color=&#39;k&#39;)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)    plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images/20d/&quot;):    os. makedirs(&quot;images/20d/&quot;)    plt. ylim(-3, 3)  plt. savefig(f&quot;images/20d/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()          for i in range(50):  plot_20d_samples(i)          !convert -delay 20 -loop 0 images/20d/*. jpg 20d. gif     From the animation above, we can see different family of functions of mean zero across these 20 points. We're really getting close now! Let us now condition on a few elements&#182;: We will create a new ordering of these variables such that the known variables occur towards the end. This allows for easy calculations for conditioning.       order = [2, 3, 4, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0, 1, 5, 10]          new_C = np. zeros_like(C)          old_order = range(20)          for i in range(20):  for j in range(20):    new_C[i, j] = C[order[i], order[j]]          sns. heatmap(new_C, xticklabels=order, yticklabels=order, cmap=&#39;jet&#39;)  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d48a86d8&gt;  Now, we can condition on (x1 = 1, x2 = 3, x6 = -3, X11 = 1). We will use the same procedure we used above in the case of 5d.       B = np. array([1, 3, -3, 1]). reshape(-1, 1)B  array([[ 1],    [ 3],    [-3],    [ 1]])        sigma_AA_20d = new_C[:-B. size, :-B. size]sigma_AA_20d. shape  (16, 16)        sigma_BB_20d = new_C[-B. size:, -B. size:]sigma_BB_20d. shape  (4, 4)        sigma_AB_20d = new_C[:-B. size, -B. size:]sigma_AB_20d. shape  (16, 4)        sigma_BA_20d = new_C[-B. size:, :-B. size]sigma_BA_20d. shape  (4, 16)        mu_bar_20d = np. zeros((20-B. size, 1)) + sigma_AB_20d@np. linalg. inv(sigma_BB_20d)@(B)          sigma_bar_20d = sigma_AA_20d - sigma_AB_20d@np. linalg. inv(sigma_BB_20d)@sigma_BA_20d          sns. heatmap(sigma_bar_20d, xticklabels=order[:-B. size], yticklabels=order[:-B. size], cmap=&#39;jet&#39;)  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d9a90e48&gt;        def plot_20d_samples_known_x(random_num):  fig, ax = plt. subplots(figsize=(10, 3))      F = multivariate_normal(mu_bar_20d. flatten(), sigma_bar_20d)  random_point = F. rvs(random_state=random_num)  index = [f&#39;X{i+1}&#39; for i in order[:-B. size]]  data_array = pd. Series(random_point, index=index)  data_array[&#39;X1&#39;] = 1  data_array[&#39;X2&#39;] = 3  data_array[&#39;X6&#39;] = -3  data_array[&#39;X11&#39;] = -1    data_array = data_array[[f&#39;X{i+1}&#39; for i in range(20)]]  data_array. plot(ax=ax, kind=&#39;line&#39;, marker=&#39;. &#39;,color=&#39;k&#39;)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  plt. scatter([0, 1,5, 10], [1, 3, -3, -1], color=&#39;red&#39;,s=100)  plt. suptitle(f&quot;Random state = {random_num}&quot;, y=1. 1)  plt. tight_layout()  import os  if not os. path. exists(&quot;images/20d/conditional/&quot;):    os. makedirs(&quot;images/20d/conditional/&quot;)  plt. grid()  plt. ylim(-4, 4)  plt. savefig(f&quot;images/20d/conditional/{random_num}. jpg&quot;, bbox_inches=&quot;tight&quot;)  plt. close()          for i in range(50):  plot_20d_samples_known_x(i)          !convert -delay 20 -loop 0 images/20d/conditional/*. jpg 20d-conditional. gif     From the plot above, we can see the known points in red and the other points wiggle to show the families of functions that we fit. Let us now draw a lot of samples and plot the mean and variance in these samples for the unknown X variables. We could have obtained the mean and variance directly using Gaussian marginalisation, but, for now let us just draw many samples.       F = multivariate_normal(mu_bar_20d. flatten(), sigma_bar_20d)dfs = {}for random_num in range(100):  random_point = F. rvs(random_state=random_num)  index = [f&#39;X{i+1}&#39; for i in order[:-B. size]]  data_array = pd. Series(random_point, index=index)  data_array[&#39;X1&#39;] = 1  data_array[&#39;X2&#39;] = 3  data_array[&#39;X6&#39;] = -3  data_array[&#39;X11&#39;] = -1    data_array = data_array[[f&#39;X{i+1}&#39; for i in range(20)]]  dfs[random_num] = data_array          fig, ax = plt. subplots(figsize=(10, 3))pd. DataFrame(dfs). mean(axis=1). plot(yerr=pd. DataFrame(dfs). std(axis=1),marker=&#39;o&#39;, color=&#39;k&#39;)plt. xticks(np. arange(len(data_array. index)), data_array. index. values)plt. scatter([0, 1,5, 10], [1, 3, -3, -1], color=&#39;red&#39;,s=100)format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d37b2358&gt;  From the plot above, we can see the uncertainty (standard deviation) and the mean values for different variables. As expected, the uncertainty close to the known points (red) is low. Also, owing to the smooth nature of the covariance function we can see the means of unknown points close to known points are fairly similar. To summarise: We can very clearly see that there is low variance in zones where we have the known values and high variance otherwise. The farther we go away from a known value, the more is the variance! Kernels!&#182;: We will now take a small plunge into the world of kernels. As mentioned earlier, we will limit the discussion to generating to covariance matrix. We will be redefining the function mentioned above to include two parameters l and s s is the scale of variancel is the influence of the point to neighbouring points      def sig(x1, x2, l, s):  return s**2*(np. exp((-1/2*(l**2))*((x1-x2)**2)))          Cov_matrix = np. zeros((100, 100))          fig, ax = plt. subplots(ncols=4, sharex=True, sharey=True)s = 1for ix, l in enumerate([0. 001, 0. 01, 0. 1, 1]):  for i in range(100):    for j in range(100):      Cov_matrix[i, j] = sig(i, j, l, 1)  im = ax[ix]. imshow(Cov_matrix, cmap=&#39;jet&#39;)  ax[ix]. set_title(f&quot;l={l}&quot;)fig. subplots_adjust(right=0. 8)cbar_ax = fig. add_axes([0. 85, 0. 35, 0. 05, 0. 3])fig. colorbar(im, cax=cbar_ax)plt. suptitle(f&quot;Covariance matrix for varying l and s = {s}&quot;)  Text(0. 5, 0. 98, &#39;Covariance matrix for varying l and s = 1&#39;)  In the plot above, we can the covariance matrices for fixed s=1 and varying l. It can be seen that for very low l, the correlations between far away points is also significant. At l=1, this ceases to be the case.       fig, ax = plt. subplots(ncols=4, sharex=True, sharey=True, figsize=(12, 3))for ix, s in enumerate([1, 10, 20, 30]):  for i in range(100):    for j in range(100):      Cov_matrix[i, j] = sig(i, j, 0. 1, s)  sns. heatmap(Cov_matrix, cmap=&#39;jet&#39;, ax=ax[ix])  ax[ix]. set_title(f&quot;s={s}&quot;)plt. suptitle(&quot;Covariance matrix for varying s and l = 0. 1&quot;)  Text(0. 5, 0. 98, &#39;Covariance matrix for varying s and l = 0. 1&#39;)  Ok, this is great. We can see the different scales on the colorbars with increasing s and fixing l Now, let us try and redo the 20 point dataset with varying kernel parameters with conditioning on some known data.       def fit_plot_gp(kernel_s, kernel_l, known_data, total_data_points, save=False):  &quot;&quot;&quot;  kernel_s: sigma^2 param of kernel  kernel_l: l (width) param of kernel  known_data: {pos: value}  total_data_points  &quot;&quot;&quot;  o = list(range(20))  for key in known_data. keys():    o. remove(key)  o. extend(list(known_data. keys()))    C = np. zeros((total_data_points, total_data_points))  for i in range(total_data_points):    for j in range(total_data_points):      C[i, j] = sig(i, j, kernel_l, kernel_s)        # Making known variables shift  new_C = np. zeros_like(C)  for i in range(20):    for j in range(20):      new_C[i, j] = C[o[i], o[j]]  B = np. array(list(known_data. values())). reshape(-1, 1)    sigma_BA_20d = new_C[-B. size:, :-B. size]  sigma_AB_20d = new_C[:-B. size, -B. size:]  sigma_BB_20d = new_C[-B. size:, -B. size:]  sigma_AA_20d = new_C[:-B. size, :-B. size]  mu_bar_20d = np. zeros((20-B. size, 1)) + sigma_AB_20d@np. linalg. inv(sigma_BB_20d)@(B)  sigma_bar_20d = sigma_AA_20d - sigma_AB_20d@np. linalg. inv(sigma_BB_20d)@sigma_BA_20d  F = multivariate_normal(mu_bar_20d. flatten(), sigma_bar_20d)  dfs = {}  for random_num in range(100):    random_point = F. rvs(random_state=random_num)    index = [f&#39;X{i+1}&#39; for i in o[:-B. size]]    data_array = pd. Series(random_point, index=index)    for k, v in known_data. items():      data_array[f&#39;X{k+1}&#39;] = v        data_array = data_array[[f&#39;X{i+1}&#39; for i in range(20)]]    dfs[random_num] = data_array  fig, ax = plt. subplots(figsize=(10, 3))  mean_vector = pd. DataFrame(dfs). mean(axis=1)  mean_vector. plot(marker=&#39;. &#39;, color=&#39;k&#39;)  yerr=pd. DataFrame(dfs). std(axis=1)    plt. fill_between(range(len(mean_vector)), mean_vector+yerr, mean_vector-yerr, color=&#39;gray&#39;,alpha=0. 4)  plt. xticks(np. arange(len(data_array. index)), data_array. index. values)  plt. scatter(list(known_data. keys()), list(known_data. values()), color=&#39;gray&#39;,s=200,zorder=1)  format_axes(plt. gca())  plt. title(f&quot; l = {kernel_l} and s = {kernel_s}&quot;)  import os  if save:    if not os. path. exists(&quot;images/20d/conditional-points/&quot;):      os. makedirs(&quot;images/20d/conditional-points/&quot;)    plt. grid()    plt. xticks(np. arange(len(data_array. index)), np. arange(len(data_array. index)))    plt. ylim(-4, 4)    plt. title(f&quot;Known data: {known_data}&quot;)    plt. savefig(f&quot;images/20d/conditional-points/{len(known_data. keys())}. jpg&quot;, bbox_inches=&quot;tight&quot;)    plt. close()              known_d = {0:-2, 1:3, 9:-1, 14:-1}          fit_plot_gp(1, 0. 5, known_d, 20)    The above plot shows the uncertainty and the family of functions for l=0. 5 and s=1.       fit_plot_gp(5, 0. 5, known_d, 20)    Keeping l=0. 5, the above plot shows how increasing s increases the uncertainty of estimation.       fit_plot_gp(1, 1, known_d, 20)    The above plot shows how increasing l reduces the influence between far away points.       fit_plot_gp(1, 100, known_d, 20)    The above plot increases l to a very large value. Seems to be just moving around the mean?       np. random. seed(0)order_points_added = np. random. choice(range(20), size=9, replace=False)k = {}for i in range(9):  k[order_points_added[i]] = np. random. choice(range(-3, 3))  fit_plot_gp(1, 0. 5, k, 20, True)          !convert -delay 40 -loop 0 images/20d/conditional-points/*. jpg 20d-conditional-main. gif    Let us create a small animation where we keep on adding points and see how the uncertainty and estimation changes Creating a scikit-learn like function containing fit and predict&#182;: I'll now bring in the formal definitions, summarise the discussion and write a function akin to scikit-learn which can accept train data to estimate for test data. Formally defining GPs&#182;: A Gaussian process is fully specified by a mean function m(x) andcovariance function K(x, x'): $$f(x) \sim GP (m(x),K(x, x')$$Let us consider a case of noiseless GPs now Noiseless GPs&#182;: Given train data $$D = {(x_i, y_i), i = 1:N}$$ Given a test set $X_{*}$ of size $N_* \times d $ containing $N_*$ points in ${\rm I\!R}^d$, we want to predict function outputs $y_{*}$ We can write: $$\begin{pmatrix} y \\ y_*\end{pmatrix} \sim \mathcal{N} \left( \begin{pmatrix} \mu \\ \mu_*\end{pmatrix} , \begin{pmatrix} K &amp; K_* \\ K_*^T &amp; K_{**}\end{pmatrix} \right)$$where $$K = Ker(X, X) \in {\rm I\!R}^{N\times N}\\K_* = Ker(X, X_*) \in {\rm I\!R}^{N\times N_*}\\K_{**} = Ker(X_*, X_*) \in {\rm I\!R}^{N_*\times N_*}\\$$We had previously used the kernel which we will continue to use def sig(x1, x2, l, s):  return s**2*(np. exp((-1/2*(l**2))*((x1-x2)**2)))We can then write: $$p(y_*|X_*, X, y) \sim \mathcal{N}(\mu', \Sigma') \\\mu' = \mu_* + K_*^TK^{-1}(x-\mu) \\\Sigma' = K_{**} - K_*^TK^{-1}K_*$$      class NoiselessGP_inversion:  def __init__(self, l=0. 1, s=1, prior_mean=0):    self. l = l    self. s = s       self. prior_mean = prior_mean      def prior_sample(self, x, n):    &quot;&quot;&quot;    Sample GP on x    &quot;&quot;&quot;    self. sample_k = self. create_cov_matrix(x, x, self. l, self. s)    for i in range(n):      pass       def kernel(self, a, b, l, s):    &quot;&quot;&quot;    Borrowed from Nando De Freita&#39;s lecture code    https://www. cs. ubc. ca/~nando/540-2013/lectures/gp. py    &quot;&quot;&quot;    sqdist = np. sum(a**2,1). reshape(-1,1) + np. sum(b**2,1) - 2*np. dot(a, b. T)    return s**2*np. exp(-. 5 * (1/l) * sqdist)    def fit(self, train_x, train_y):    self. train_x = train_x    self. train_y = train_y    self. N = len(train_x)    self. K = self. kernel(train_x, train_x, self. l, self. s)              def predict(self, test_x):    self. N_star = len(test_x)    self. K_star = self. kernel(self. train_x, test_x, self. l, self. s)    self. K_star_star = self. kernel(test_x, test_x, self. l, self. s)    self. posterior_mu = self. prior_mean + self. K_star. T@np. linalg. inv(self. K)@(self. train_y-self. prior_mean)    self. posterior_sigma = self. K_star_star - self. K_star. T@np. linalg. inv(self. K)@self. K_star    return self. posterior_mu, self. posterior_sigma          clf = NoiselessGP_inversion()          train_x = np. array([-4, -3, -2, -1, 1]). reshape(5,1)train_y = np. sin(train_x)test_x = np. linspace(-5, 5, 50). reshape(-1, 1)test_y = np. sin(test_x)          plt. plot(train_x, train_y,&#39;ko-&#39;)  [&lt;matplotlib. lines. Line2D at 0x7fb2d954f198&gt;]        clf. fit(train_x, train_y)          posterior_mu, posterior_var = clf. predict(test_x)          plt. plot(test_x, clf. posterior_mu,&#39;k&#39;,label=&#39;Predicted&#39;,lw=1)plt. plot(test_x, test_y, &#39;purple&#39;,label=&#39;GT&#39;,lw=2)plt. plot(train_x, train_y, &#39;ko&#39;,label=&#39;Training Data&#39;)plt. fill_between(test_x. flatten(),         (clf. posterior_mu. flatten() - clf. posterior_sigma. diagonal(). flatten()),         (clf. posterior_mu. flatten() + clf. posterior_sigma. diagonal(). flatten()),         color=&#39;gray&#39;, alpha=0. 3        )plt. legend()format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d9516518&gt;  Cholesky decomposition&#182;: We had previously used matrix inversion to do the computation for computing the posterior mean and variance in our GP. However, the matrices involved may be poorly conditioned and thus Cholesky decomposition is often favoured. From Wikipedia, the Cholesky decomposition of a matrix $A$ is given as:$$\mathbf{A} = \mathbf{L L}^T$$ where $L$ is a real lower triangular matrix. We can thus re-write the posterior mean and covariance as: $$p(y_*|X_*, X, y) \sim \mathcal{N}(\mu', \Sigma') \\K = LL^T \\$$We are now going to use the \ as follows:if $A\omega = B$, then $\omega$ = $B$ \ $A$ We now have:$$\alpha = K^{-1}(x-\mu) \\or, \alpha = {LL^T}^{-1}(x-\mu) \\or, \alpha = L^{-T}L^{-1}(x-\mu) \\Let, K^{-1}(x-\mu) = \beta \\Thus, L^{-T}L^{-1}(x-\mu) = \beta \\Let, L^{-1}(x-\mu) = \gamma\\Thus, L\gamma = x-\mu \\Thus, \gamma = L \setminus (x-\mu)\\\Thus, \alpha = L^{T} \setminus (L \setminus (x-\mu))$$ In Python, the same can be written as: L = np. linalg. cholesky(K)  alpha = np. linalg. solve(L. T, np. linalg. solve(L, x-mu))Thus, we can find the posterior mean as:$$\mu' = \mu_* + K_*^T \alpha \\$$ We also know that$$\Sigma' = K_{**} - K_*^TK^{-1}K_*$$ Let us now define$$v = L \setminus K_{*}\\or, v = L^{-1}K_{*}\\Thus, v^{T} = K_{*}^TL^{-T}\\Thus, v^{T}v = K_{*}^TL^{-T}L^{-1}K_{*}\\Thus, v^{T}v = K_*^TK^{-1}K_* = K_{**} - \Sigma' $$ Let us know rewrite the code with Cholesky decomposition.       class NoiselessGP_Cholesky:  def __init__(self, l=0. 1, s=1, prior_mean=0):    self. l = l    self. s = s       self. prior_mean = prior_mean      def prior_sample(self, x, n):    &quot;&quot;&quot;    Sample GP on x    &quot;&quot;&quot;    self. sample_k = self. create_cov_matrix(x, x, self. l, self. s)    for i in range(n):      pass       def kernel(self, a, b, l, s):    &quot;&quot;&quot;    Borrowed from Nando De Freita&#39;s lecture code    https://www. cs. ubc. ca/~nando/540-2013/lectures/gp. py    &quot;&quot;&quot;    sqdist = np. sum(a**2,1). reshape(-1,1) + np. sum(b**2,1) - 2*np. dot(a, b. T)    return s**2*np. exp(-. 5 * (1/l) * sqdist)    def fit(self, train_x, train_y):    self. train_x = train_x    self. train_y = train_y    self. N = len(train_x)    self. K = self. kernel(train_x, train_x, self. l, self. s)    self. L = np. linalg. cholesky(self. K)              def predict(self, test_x):    self. N_star = len(test_x)    self. K_star = self. kernel(self. train_x, test_x, self. l, self. s)    self. K_star_star = self. kernel(test_x, test_x, self. l, self. s)    self. alpha = np. linalg. solve(self. L. T, np. linalg. solve(self. L, self. train_y-self. prior_mean))    self. v = np. linalg. solve(self. L, self. K_star)    self. posterior_mu = self. prior_mean + self. K_star. T@self. alpha    self. posterior_sigma = self. K_star_star - self. v. T@self. v    return self. posterior_mu, self. posterior_sigma          clf = NoiselessGP_Cholesky()clf. fit(train_x, train_y)posterior_mu_cholesky, posterior_var_cholesky = clf. predict(test_x)    We will now compare our Cholesky decomposition based decompostion with the earlier one.       np. allclose(posterior_mu_cholesky, posterior_mu)  True        np. allclose(posterior_var_cholesky, posterior_var)  True  Ok, all looks good till now! Let us now move on to the case for Noisy GPs. Noisy GPs&#182;: Previously, we had assumed a noiseless model, which is to say, for the observed data, we had:$$y_i = f(x_i)$$ We now make the model more flexible by saying that there can be noise in the observed data as well, thus:$$y_i = f(x_i) + \epsilon \\\epsilon \sim \mathcal{N}(0, \sigma_y^2)$$ One of the main difference compared to the noiseless model would be that in the noisy model, we will have some uncertainty even about the training points. Everything about our model remains the same, except for the change in the covariance matrix $K$ for the training points, which is now given as: $$K_y = \sigma_y^2\mathbf{I_n} + K$$We can now rewrite the function as follows:       class NoisyGP:  def __init__(self, l = 0. 1, s = 1, prior_mean = 0, sigma_y = 1):    self. l = l    self. s = s       self. prior_mean = prior_mean    self. sigma_y = sigma_y      def prior_sample(self, x, n):    &quot;&quot;&quot;    Sample GP on x    &quot;&quot;&quot;    self. sample_k = self. create_cov_matrix(x, x, self. l, self. s)    for i in range(n):      pass       def kernel(self, a, b, l, s):    &quot;&quot;&quot;    Borrowed from Nando De Freita&#39;s lecture code    https://www. cs. ubc. ca/~nando/540-2013/lectures/gp. py    &quot;&quot;&quot;    sqdist = np. sum(a**2,1). reshape(-1,1) + np. sum(b**2,1) - 2*np. dot(a, b. T)    return s**2*np. exp(-. 5 * (1/l) * sqdist)    def fit(self, train_x, train_y):    self. train_x = train_x    self. train_y = train_y    self. N = len(train_x)    self. K = self. kernel(train_x, train_x, self. l, self. s) + self. sigma_y*np. eye(len(train_x))    self. L = np. linalg. cholesky(self. K)              def predict(self, test_x):    self. N_star = len(test_x)    self. K_star = self. kernel(self. train_x, test_x, self. l, self. s)    self. K_star_star = self. kernel(test_x, test_x, self. l, self. s)    self. alpha = np. linalg. solve(self. L. T, np. linalg. solve(self. L, self. train_y-self. prior_mean))    self. v = np. linalg. solve(self. L, self. K_star)    self. posterior_mu = self. prior_mean + self. K_star. T@self. alpha    self. posterior_sigma = self. K_star_star - self. v. T@self. v    return self. posterior_mu, self. posterior_sigma          clf = NoisyGP(sigma_y=0. 2)clf. fit(train_x, train_y)posterior_mu_noisy, posterior_var_noisy = clf. predict(test_x)          plt. plot(test_x, clf. posterior_mu,&#39;k&#39;,label=&#39;Predicted&#39;,lw=1)plt. plot(test_x, test_y, &#39;purple&#39;,label=&#39;GT&#39;,lw=2)plt. plot(train_x, train_y, &#39;ko&#39;,label=&#39;Training Data&#39;)plt. fill_between(test_x. flatten(),         (clf. posterior_mu. flatten() - clf. posterior_sigma. diagonal(). flatten()),         (clf. posterior_mu. flatten() + clf. posterior_sigma. diagonal(). flatten()),         color=&#39;gray&#39;, alpha=0. 3        )plt. legend()format_axes(plt. gca())  &lt;matplotlib. axes. _subplots. AxesSubplot at 0x7fb2d4725978&gt;  We can now see that our model has some uncertainty even on the train points! "
    }, {
    "id": 6,
    "url": "https://nipunbatra.github.io/academia/2018/08/18/Placement-Preparation-2018-1-HashMap.html",
    "title": "Placement-Preparation-2018-1-HashMap",
    "body": "2018/08/18 -           In this blogpost, we will take a question from Cracking the Coding Interview. I discussed this question with Masters students at IITGN. We came up with some great answers. I'll show how we increasingly went towards better solutions starting from naive ones. Problem statement Find all integer solutions to the problem$a^3 + b^3 = c^3 + d^3$ where $1&lt;=a&lt;=n,1&lt;=b&lt;=n,1&lt;=c&lt;=n,1&lt;=d&lt;=n$ First attempt : Naive bruteforce $O(n^4)$&#182;: Let's write a very simple first attempt. We will write four nested loops. This will be $O(n^4)$ solution.       def f1(n):  out = []  for a in range(1, n+1):    for b in range(1, n+1):      for c in range(1, n+1):        for d in range(1, n+1):          if a**3 + b**3 == c**3 + d**3:            out. append((a, b, c, d))  return out           f1_time = %timeit -o f1(50)  6. 65 s ± 203 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)        f1_time. average  6. 646897936570895  Second attempt : Reduce computations in brute force method&#182;: Let's now try to optimise f1. We will still use a solution of $O(n^4)$ solution. However, we add one small optimisation fo f1. We break from the innermost loop once we find a match. This will hopefull save us some computations.       def f2(n):  out = []  for a in range(1, n+1):    for b in range(1, n+1):      for c in range(1, n+1):        for d in range(1, n+1):          if a**3 + b**3 == c**3 + d**3:            out. append((a, b, c, d))            break  return out           f2_time = %timeit -o f2(50)  6. 29 s ± 26. 3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)  Ok. We're little better than f1. Every reduced computation is time saved! Third attempt : Reduce repeated computations by saving cubes of numbers&#182;: One of the student came up with an excellent observation. Why should we keep on computing cubes of numbers? This is a repeated operation. Let's instead store them in a dictionary.       def f3(n):  cubes = {}  for x in range(1, n+1):    cubes[x] = x**3  out = []  for a in range(1, n+1):    for b in range(1, n+1):      for c in range(1, n+1):        for d in range(1, n+1):          if cubes[a] + cubes[b] == cubes[c] + cubes[d]:            out. append((a, b, c, d))            break  return out           f3_time = %timeit -o f3(50)  1. 05 s ± 4. 11 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)  Ok. We now mean business! This is about 6 times quicker than our previous version. Fourth attempt : Reduce one loop $O(n^3)$&#182;: In this solution, we will reduce one loop. We can solve for $d^3 = a^3 + b^3 - c^3$ and find all the integer solutions. Now, there's another clever optimisation that I have added. We can precompute the cubes and the cuberoots corresponding to numbers from 1 to N and perfect cubes from 1 to $N^3$ respectively.       def f4(n):  cubes = {}  cuberoots = {}  for x in range(1, n+1):    x3 = x**3    cubes[x] = x3    cuberoots[x3] = x  out = []  for a in range(1, n+1):    for b in range(1, n+1):      for c in range(1, n+1):        d3 = (cubes[a] + cubes[b] - cubes[c])        if d3 in cuberoots:          out. append((a, b, c, cuberoots[d3]))  return out           f4_time = %timeit -o f4(50)  21. 7 ms ± 1. 99 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)  This is seriously fast now! Fifth attempt : Reduce another loop $O(n^2)$&#182;: In this solution, we will reduce one more loop. We can compute $a^3 + b^3$ for all a, b. And then find c and d where $c^3 + d^3$ is the same as $a^3 + b^3$. This has a few Python tricks inside! One of the special cases to handle is of the type $1^3 + 2^3 = 2^3 + 1^3$       def f5(n):  out = []  cubes = {}  for x in range(1, n+1):    cubes[x] = x**3    sum_a3_b3 = {}  for a in range(1, n+1):    for b in range(1, n+1):      temp = cubes[a]+cubes[b]      if temp in sum_a3_b3:          sum_a3_b3[temp]. append((a, b))      else:        sum_a3_b3[temp] = [(a, b)]  for c in range(1, n+1):    for d in range(1, n+1):      sum_c3_d3 = cubes[c] + cubes[d]      if sum_c3_d3 in sum_a3_b3:        for (a, b) in sum_a3_b3[sum_c3_d3]:          out. append((a, b, c, d))  return out          f5_time = %timeit -o f5(50)  1. 97 ms ± 235 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)  Plain Wow! Going from 6 seconds to about 2 ms! Let's plot the timings on a log scale to learn more.       %matplotlib inlineimport matplotlib. pyplot as pltimport pandas as pd          s = pd. Series({&#39;Naive (O(N^4))&#39;:f1_time. average,       &#39;Naive (O(N^4)) with break&#39;:f2_time. average,       &#39;Naive (O(N^4)) with break and precomputing cubes&#39;:f3_time. average,       &#39;(O(N^3))&#39;:f4_time. average,       &#39;(O(N^2))&#39;:f5_time. average})          s. plot(kind=&#39;bar&#39;, logy=True)plt. ylabel(&quot;Time&quot;);    Hope this was fun! "
    }, {
    "id": 7,
    "url": "https://nipunbatra.github.io/academia/2018/08/11/academics-time.html",
    "title": "An illustrative view of how academics spend their time!",
    "body": "2018/08/11 -  "
    }, {
    "id": 8,
    "url": "https://nipunbatra.github.io/sustainability/2018/06/26/map-electricity-access.html",
    "title": "Visualising Electricity Access Over Space and Time",
    "body": "2018/06/26 -           In this post, I'll explore electricity access, i. e. globally what fraction of people have access to electricity. Beyond the goal of finding the electricity access, this post will also serve to illustrate how the coolness coefficient of the Python visualisation ecosystem! I'll be using data from World Bank for electricity access. See the image below for the corresponding page.  Downloading World Bank data&#182;: Now, a Python package called wbdata provides a fairly easy way to access World Bank data. I'd be using it to get data in Pandas DataFrame.       %matplotlib inlineimport pandas as pdimport wbdataimport matplotlib. pyplot as pltimport datetimedata_date = (datetime. datetime(1990, 1, 1), datetime. datetime(2016, 1, 1))df_elec = wbdata. get_data(&quot;EG. ELC. ACCS. ZS&quot;, pandas=True, data_date=data_date)          df_elec. head()  country   dateArab World 2016  88. 768654      2015  88. 517967      2014  88. 076774      2013  88. 389705      2012  87. 288244Name: value, dtype: float64  Downloading Geodata and Reading Using GeoPandas&#182;: I'd now be downloading shapefile data for different countries. This will help us to spatially plot the data for the different countries.       !wget http://naciscdn. org/naturalearth/10m/cultural/ne_10m_admin_0_countries_lakes. zip  --2018-06-26 15:52:50-- http://naciscdn. org/naturalearth/10m/cultural/ne_10m_admin_0_countries_lakes. zipResolving naciscdn. org (naciscdn. org). . . 146. 201. 97. 163Connecting to naciscdn. org (naciscdn. org)|146. 201. 97. 163|:80. . . connected. HTTP request sent, awaiting response. . . 200 OKLength: 5077755 (4. 8M) [application/x-zip-compressed]Saving to: ‘ne_10m_admin_0_countries_lakes. zip’ne_10m_admin_0_coun 100%[===================&gt;]  4. 84M  246KB/s  in 22s   2018-06-26 15:53:12 (228 KB/s) - ‘ne_10m_admin_0_countries_lakes. zip’ saved [5077755/5077755]  Extracting shapefile&#182;:       import zipfilezip_ref = zipfile. ZipFile(&#39;ne_10m_admin_0_countries_lakes. zip&#39;, &#39;r&#39;)zip_ref. extractall(&#39;. &#39;)zip_ref. close()          import geopandas as gpdgdf = gpd. read_file(&#39;ne_10m_admin_0_countries_lakes. shp&#39;)[[&#39;ADM0_A3&#39;, &#39;geometry&#39;]]          gdf. head()           ADM0_A3   geometry         0   IDN   (POLYGON ((117. 7036079039552 4. 163414542001791. . .        1   MYS   (POLYGON ((117. 7036079039552 4. 163414542001791. . .        2   CHL   (POLYGON ((-69. 51008875199994 -17. 506588197999. . .        3   BOL   POLYGON ((-69. 51008875199994 -17. 5065881979999. . .        4   PER   (POLYGON ((-69. 51008875199994 -17. 506588197999. . .      Visualising electricity access in 2016&#182;: Getting electricity access data for 2016&#182;:       df_2016 = df_elec. unstack()[[&#39;2016&#39;]]. dropna()          df_2016. head()        date   2016       country            Afghanistan   84. 137138       Albania   100. 000000       Algeria   99. 439568       Andorra   100. 000000       Angola   40. 520607     In order to visualise electricity access data over the map, we would have to join the GeoPandas object gdf and df_elec Joining gdf and df_2016&#182;: Now, gdf uses alpha_3 codes for country names like AFG, etc. , whereas df_2016 uses country names. We will thus use pycountry package to get code names corresponding to countries in df_2016 as shown in this StackOverflow post.       import pycountrycountries = {}for country in pycountry. countries:  countries[country. name] = country. alpha_3codes = [countries. get(country, &#39;Unknown code&#39;) for country in df_2016. index]df_2016[&#39;Code&#39;] = codes          df_2016. head()        date   2016   Code       country               Afghanistan   84. 137138   AFG       Albania   100. 000000   ALB       Algeria   99. 439568   DZA       Andorra   100. 000000   AND       Angola   40. 520607   AGO     Now, we can join the two data sources       merged_df_2016 = gpd. GeoDataFrame(pd. merge(gdf, df_2016, left_on=&#39;ADM0_A3&#39;, right_on=&#39;Code&#39;))          merged_df_2016. head()           ADM0_A3   geometry   2016   Code         0   IDN   (POLYGON ((117. 7036079039552 4. 163414542001791. . .    97. 620000   IDN       1   MYS   (POLYGON ((117. 7036079039552 4. 163414542001791. . .    100. 000000   MYS       2   CHL   (POLYGON ((-69. 51008875199994 -17. 506588197999. . .    100. 000000   CHL       3   PER   (POLYGON ((-69. 51008875199994 -17. 506588197999. . .    94. 851746   PER       4   ARG   (POLYGON ((-68. 4486097329999 -52. 3466170159999. . .    100. 000000   ARG     Finally plotting!&#182;:       # Example borrowed from http://ramiro. org/notebook/geopandas-choropleth/cmap=&#39;OrRd&#39;figsize = (16, 5)ax = merged_df_2016. plot(column=&#39;2016&#39;, cmap=cmap, figsize=figsize,legend=True)title = &#39;Electricity Access(% of population) in {}&#39;. format(&#39;2016&#39;)gdf[~gdf. ADM0_A3. isin(merged_df_2016. ADM0_A3)]. plot(ax=ax, color=&#39;#fffafa&#39;, hatch=&#39;///&#39;)ax. set_title(title, fontdict={&#39;fontsize&#39;: 15}, loc=&#39;left&#39;)ax. set_axis_off()    Creating animation for access across time&#182;:       !mkdir -p elec_access          def save_png_year(year, path=&quot;elec_access&quot;):  df_year = df_elec. unstack()[[&#39;{}&#39;. format(year)]]. dropna()  codes = [countries. get(country, &#39;Unknown code&#39;) for country in df_year. index]  df_year[&#39;Code&#39;] = codes  merged_df_year = gpd. GeoDataFrame(pd. merge(gdf, df_year, left_on=&#39;ADM0_A3&#39;, right_on=&#39;Code&#39;))  figsize = (16, 5)  ax = merged_df_year. plot(column=&#39;{}&#39;. format(year), cmap=cmap, figsize=figsize,legend=True,vmin=0. 0, vmax=100. 0)  title = &#39;Electricity Access(% of population) in {}&#39;. format(year)  gdf[~gdf. ADM0_A3. isin(merged_df_year. ADM0_A3)]. plot(ax=ax, color=&#39;#fffafa&#39;, hatch=&#39;///&#39;)  ax. set_title(title, fontdict={&#39;fontsize&#39;: 15}, loc=&#39;left&#39;)  ax. set_axis_off()  plt. savefig(&#39;{}/{}. png&#39;. format(path, year), dpi=300)  plt. close()          for year in range(1990, 2017):  save_png_year(year)          # Borrowed from http://www. kevinwampler. com/blog/2016/09/10/creating-animated-gifs-using-python. htmldef create_gifv(input_files, output_base_name, fps):  import imageio  output_extensions = [&quot;gif&quot;]  input_filenames = [&#39;elec_access/{}. png&#39;. format(year) for year in range(1990, 2017)]  poster_writer = imageio. get_writer(&quot;{}. png&quot;. format(output_base_name), mode=&#39;i&#39;)  video_writers = [    imageio. get_writer(&quot;{}. {}&quot;. format(output_base_name, ext), mode=&#39;I&#39;, fps=fps)    for ext in output_extensions]  is_first = True  for filename in input_filenames:    img = imageio. imread(filename)    for writer in video_writers:      writer. append_data(img)    if is_first:      poster_writer. append_data(img)    is_first = False  for writer in video_writers + [poster_writer]:    writer. close()          create_gifv(&quot;elec_access/*. png&quot;, &quot;electricity_access&quot;, 4)     Across Africa and SE Asia, one can clearly see a gradual improvement in access! Hope you had fun reading this post :) "
    }, {
    "id": 9,
    "url": "https://nipunbatra.github.io/air%20quality/2018/06/21/aq-india-map.html",
    "title": "Mapping location of air quality sensing in India",
    "body": "2018/06/21 -           In this notebook, I'll show a quick example of how to use Folium (which internally uses LeafletJS) for visualising the location of air quality monitors in India. The purpose of this notebook is eductional in nature. Standard Imports&#182;:       import numpy as npimport matplotlib. pyplot as pltimport pandas as pd%matplotlib inline    Downloading data from OpenAQ for 2018-04-06&#182;:       !wget --no-check-certificate https://openaq-data. s3. amazonaws. com/2018-04-06. csv -P /Users/nipun/Downloads/  --2020-02-29 17:52:50-- https://openaq-data. s3. amazonaws. com/2018-04-06. csvResolving openaq-data. s3. amazonaws. com (openaq-data. s3. amazonaws. com). . . 52. 216. 99. 123Connecting to openaq-data. s3. amazonaws. com (openaq-data. s3. amazonaws. com)|52. 216. 99. 123|:443. . . connected. WARNING: cannot verify openaq-data. s3. amazonaws. com&#39;s certificate, issued by ‘CN=DigiCert Baltimore CA-2 G2,OU=www. digicert. com,O=DigiCert Inc,C=US’: Unable to locally verify the issuer&#39;s authority. HTTP request sent, awaiting response. . . 200 OKLength: 133839107 (128M) [text/csv]Saving to: ‘/Users/nipun/Downloads/2018-04-06. csv. 1’2018-04-06. csv. 1   37%[======&gt;       ] 47. 37M 3. 79MB/s  eta 40s  ^C        import pandas as pddf = pd. read_csv(&quot;/Users/nipun/Downloads/2018-04-06. csv&quot;)df = df[(df. country==&#39;IN&#39;)&amp;(df. parameter==&#39;pm25&#39;)]. dropna(). groupby(&quot;location&quot;). mean()          df           value   latitude   longitude       location                  Adarsh Nagar, Jaipur - RSPCB   79. 916667   26. 902909   75. 836853       Anand Kala Kshetram, Rajamahendravaram - APPCB   42. 750000   16. 987287   81. 736318       Ardhali Bazar, Varanasi - UPPCB   103. 666667   25. 350599   82. 908307       Asanol Court Area, Asanol - WBPCB   56. 833333   23. 685297   86. 945968       Ashok Nagar, Udaipur - RSPCB   114. 750000   24. 588617   73. 632140       . . .    . . .    . . .    . . .        Vasundhara, Ghaziabad, UP - UPPCB   223. 333333   28. 660335   77. 357256       Vikas Sadan, Gurgaon, Haryana - HSPCB   280. 250000   28. 450124   77. 026305       Vindhyachal STPS, Singrauli - MPPCB   144. 000000   24. 108970   82. 645580       Ward-32 Bapupara, Siliguri - WBPCB   195. 000000   26. 688305   88. 412668       Zoo Park, Hyderabad - TSPCB   82. 500000   17. 349694   78. 451437   79 rows × 3 columns   Downloading World GeoJson file&#182;:       !wget --no-check-certificate https://raw. githubusercontent. com/python-visualization/folium/master/examples/data/world-countries. json  --2020-02-29 17:53:17-- https://raw. githubusercontent. com/python-visualization/folium/master/examples/data/world-countries. jsonResolving raw. githubusercontent. com (raw. githubusercontent. com). . . 151. 101. 8. 133Connecting to raw. githubusercontent. com (raw. githubusercontent. com)|151. 101. 8. 133|:443. . . connected. WARNING: cannot verify raw. githubusercontent. com&#39;s certificate, issued by ‘CN=DigiCert SHA2 High Assurance Server CA,OU=www. digicert. com,O=DigiCert Inc,C=US’: Unable to locally verify the issuer&#39;s authority. HTTP request sent, awaiting response. . . 200 OKLength: 252515 (247K) [text/plain]Saving to: ‘world-countries. json’world-countries. jso 100%[===================&gt;] 246. 60K  376KB/s  in 0. 7s  2020-02-29 17:53:19 (376 KB/s) - ‘world-countries. json’ saved [252515/252515]  Creating india. json correspdonding to Indian data&#182;:       import jsone = json. load(open(&#39;world-countries. json&#39;,&#39;r&#39;))json. dump(e[&#39;features&#39;][73], open(&#39;india. json&#39;,&#39;w&#39;))          import foliumfolium_map = folium. Map(width = &#39;60%&#39;,height=800,location=[20, 77],            zoom_start=5,            tiles=&quot;Stamen Terrain&quot;,min_lat=7, max_lat=35, min_lon=73, max_lon=90)for x in df. iterrows():  name = x[0]  lat, lon = x[1][&#39;latitude&#39;], x[1][&#39;longitude&#39;]  folium. CircleMarker([lat, lon], radius=5, color=&#39;#000000&#39;,fill_color=&#39;#D3D3D3&#39; , fill_opacity=1). add_to(folium_map)folium. GeoJson(&#39;india. json&#39;). add_to(folium_map)folium_map  &lt;iframe src= about:blank  style= position:absolute;width:100%;height:100%;left:0;top:0;border:none !important;  data-html=<!DOCTYPE html>
<head>    
    <meta http-equiv="content-type" content="text/html; charset=UTF-8" />
    
        <script>
            L_NO_TOUCH = false;
            L_DISABLE_3D = false;
        </script>
    
    <script src="https://cdn.jsdelivr.net/npm/leaflet@1.5.1/dist/leaflet.js"></script>
    <script src="https://code.jquery.com/jquery-1.12.4.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.js"></script>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.5.1/dist/leaflet.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.2.0/css/bootstrap-theme.min.css"/>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css"/>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Leaflet.awesome-markers/2.0.2/leaflet.awesome-markers.css"/>
    <link rel="stylesheet" href="https://rawcdn.githack.com/python-visualization/folium/master/folium/templates/leaflet.awesome.rotate.css"/>
    <style>html, body {width: 100%;height: 100%;margin: 0;padding: 0;}</style>
    <style>#map {position:absolute;top:0;bottom:0;right:0;left:0;}</style>
    
            <meta name="viewport" content="width=device-width,
                initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />
            <style>
                #map_60c2bfe17360488ebe908eecf9e486f8 {
                    position: relative;
                    width: 60.0%;
                    height: 800.0px;
                    left: 0.0%;
                    top: 0.0%;
                }
            </style>
        
</head>
<body>    
    
            <div class="folium-map" id="map_60c2bfe17360488ebe908eecf9e486f8" ></div>
        
</body>
<script>    
    
            var map_60c2bfe17360488ebe908eecf9e486f8 = L.map(
                "map_60c2bfe17360488ebe908eecf9e486f8",
                {
                    center: [20.0, 77.0],
                    crs: L.CRS.EPSG3857,
                    zoom: 5,
                    zoomControl: true,
                    preferCanvas: false,
                }
            );

            

        
    
            var tile_layer_26fe9016e0404447ae57c5a4a22e633e = L.tileLayer(
                "https://stamen-tiles-{s}.a.ssl.fastly.net/terrain/{z}/{x}/{y}.jpg",
                {"attribution": "Map tiles by \u003ca href=\"http://stamen.com\"\u003eStamen Design\u003c/a\u003e, under \u003ca href=\"http://creativecommons.org/licenses/by/3.0\"\u003eCC BY 3.0\u003c/a\u003e. Data by \u0026copy; \u003ca href=\"http://openstreetmap.org\"\u003eOpenStreetMap\u003c/a\u003e, under \u003ca href=\"http://creativecommons.org/licenses/by-sa/3.0\"\u003eCC BY SA\u003c/a\u003e.", "detectRetina": false, "maxNativeZoom": 18, "maxZoom": 18, "minZoom": 0, "noWrap": false, "opacity": 1, "subdomains": "abc", "tms": false}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_d8ff35eda7214585bdf69acb6660898f = L.circleMarker(
                [26.902909000000005, 75.836853],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_fa9f8b1755534709abf2699d5a6292e1 = L.circleMarker(
                [16.9872867, 81.73631759999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_f6bff631fa4e4db8b81883ef37d482ca = L.circleMarker(
                [25.35059860000001, 82.9083074],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_494663d990ca41439b54a538e5e3c4b9 = L.circleMarker(
                [23.685296999999995, 86.94596800000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_ca9e756c085946829135d30fbe8374d8 = L.circleMarker(
                [24.588616599999998, 73.63213970000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_f9c547de0f1044b08cc4316f217a17d6 = L.circleMarker(
                [19.041846999999997, 72.86551299999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_317581b322924979a4b6b8a80326cae7 = L.circleMarker(
                [22.968259100000008, 76.06411800000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_6d6df831f9de4c13aef699f91f8b80fb = L.circleMarker(
                [17.540891000000002, 78.358528],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_29ab6099105a442ca66f491ed7104d62 = L.circleMarker(
                [28.725650400000003, 77.20115729999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_6dd26ae37e4b4d1bb7a1b3cd36ac535d = L.circleMarker(
                [28.551200499999997, 77.27357370000003],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_3dec0c687f80474cae13badc7992a455 = L.circleMarker(
                [26.882100299999994, 80.93027529999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_23642af3669f4aab9481a50c34160c16 = L.circleMarker(
                [17.460103, 78.33436099999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_36ec957c2f4845a2b07b0b40369c81b2 = L.circleMarker(
                [19.645323999999995, 77.6345232],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_50209871162e430a9a945c52e3c1e8c1 = L.circleMarker(
                [31.321907000000007, 75.57891399999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_f3ae5b6b2f034145b18392895a0d854c = L.circleMarker(
                [26.268248999999997, 73.01938529999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_94e58e8af6b84da08256e2b4141d0e7b = L.circleMarker(
                [28.750049900000004, 77.11126149999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_075a01fa374a4c1694f1af86f188040d = L.circleMarker(
                [21.8004996, 83.8396977],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_d9af987460a44d2a978a25acbb8c7641 = L.circleMarker(
                [17.72, 83.29999999999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_cef11c28ae8643d0946b78e5a9477196 = L.circleMarker(
                [20.0073285, 73.7762427],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_c582f9c23ab74c2f912219771a92789a = L.circleMarker(
                [31.62, 74.87651200000002],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_ea99e975e42649c194fd9e1f0dc3fdff = L.circleMarker(
                [17.4342359, 78.4170318],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_8a94b86df4414740992f5e69c3aa2163 = L.circleMarker(
                [17.5316895, 78.21893899999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_cf58e7a98be14147820f0fb0b687fd34 = L.circleMarker(
                [28.5627763, 77.11800530000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_478dca6f4c4c4023aaaa11a90d862e1b = L.circleMarker(
                [25.5941, 85.1376],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_ba7e203b711b4220b9eaef7717476cce = L.circleMarker(
                [28.680274699999988, 77.20115729999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_cf89037a7b8a4298877f294ee5a85f67 = L.circleMarker(
                [13.005218899999997, 80.23981249999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_9e01c2b738f943d0b99c0f3bcc5d536d = L.circleMarker(
                [28.6316945, 77.24943870000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_35ae41168bac4c4483e1933184bfab49 = L.circleMarker(
                [25.771060999999992, 73.34022700000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_8779664f9f0848e3ad3757d9d3350bf3 = L.circleMarker(
                [18.501174300000002, 73.8165527],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_8abeeaa2980e44849f68681eb0a5216e = L.circleMarker(
                [28.825340999999998, 78.7213009],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_f44ca6af0a314074bd956b4bd14eea99 = L.circleMarker(
                [26.845880500000003, 80.93655409999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_65fd07638d264d4d8eb9f009afcb4d70 = L.circleMarker(
                [28.591824500000012, 77.22730739999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_6b97336fa8ec46aea0cdd56b0507c230 = L.circleMarker(
                [23.182718999999995, 75.76821800000002],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_49603c28f4ec4c3cb1374217d1142d1c = L.circleMarker(
                [13.164544000000005, 80.26284999999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_8b70bfa14a404307aaac576a643ac603 = L.circleMarker(
                [28.637268800000005, 77.2005604],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_c8d9553607e74c12ac0c2e0e7fc1699b = L.circleMarker(
                [23.002657, 72.591912],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_43d28573b01149bcaf123815cc55547d = L.circleMarker(
                [30.349388, 76.366642],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_9aa2def6ffbf43cdbc12f38c2c1b0016 = L.circleMarker(
                [19.8389439, 75.244448],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_d1af1cfff3ac4ffda6ca4b882f4c1cd9 = L.circleMarker(
                [26.120900000000002, 85.36469999999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_1dc08906af0f450695c8cc75f142a758 = L.circleMarker(
                [28.60909, 77.0325413],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_23d0dd8165a74410a7f47c9cd696d0b9 = L.circleMarker(
                [26.4703136, 80.32298630000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_a270f656d823450884dad9dc76039b1c = L.circleMarker(
                [26.871427999999998, 80.95714499999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_ea74ceacb1394f439b10db197539f930 = L.circleMarker(
                [28.657381400000006, 77.15854470000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_df2783b58fca458180c899af96c4a16a = L.circleMarker(
                [21.152874999999998, 79.05175310000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_401a9ce79f9f4c1aa24a7648121823c2 = L.circleMarker(
                [22.568731899999992, 88.2797276],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_9f33f1585441478d8b44b50d1530c74c = L.circleMarker(
                [13.0270199, 77.494094],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_28f6fb21011741759add6e50c7870e5c = L.circleMarker(
                [19.192056000000004, 72.9585188],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_af32bad39a644332bd25826613ec203f = L.circleMarker(
                [8.514909300000001, 76.94358790000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_917e4227855344bc85eafa0c78b081e1 = L.circleMarker(
                [26.916409199999993, 75.79949009999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_d6860aecfdf543c8a2a0a9147b72d587 = L.circleMarker(
                [30.90280000000001, 75.80859999999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_e9d39e20adb74b439d8a874791d1c3a7 = L.circleMarker(
                [28.61030399999999, 77.0996943],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_1cfde93b704746abae25c6b40fae6d50 = L.circleMarker(
                [28.564610199999997, 77.1670103],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_23a7b397368d496f8312a55cac8eda27 = L.circleMarker(
                [28.194909000000003, 76.862296],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_5278156045164ca685598a5e74604e2b = L.circleMarker(
                [30.649961, 76.33144200000002],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_ef3b039e211247da9c89d709a7ec9815 = L.circleMarker(
                [17.4559458, 78.43321519999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_b92a6659a5de48b484f4f22f7fbaaed3 = L.circleMarker(
                [27.19865833, 78.00598056],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_00317129bb4d48f28bf07c1ec61520e1 = L.circleMarker(
                [16.5150833, 80.5181667],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_f0c7a95812d0454593258fd70886a605 = L.circleMarker(
                [28.5447608, 77.32312569999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_3196f3650d6f4921a4a254f0a339f610 = L.circleMarker(
                [28.624547899999996, 77.35771039999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_8ce9c3ec0da540b8844d993a89608814 = L.circleMarker(
                [28.40884210000001, 77.3099081],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_c710642a4db34f50afbfbdff17ca41db = L.circleMarker(
                [22.624757999999996, 75.67523800000002],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_7f4f77d4cbc64443895cb52302b776f1 = L.circleMarker(
                [30.705777800000003, 76.85318055555548],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_60c1b46c6bf845888f7b2aee3d8ca990 = L.circleMarker(
                [23.108439999999998, 77.51142800000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_89f562b2443c4de684f06848279daa98 = L.circleMarker(
                [28.651478100000006, 77.1473105],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_780169482bcd4a17a44d1c177707b059 = L.circleMarker(
                [26.950292900000004, 75.73094300000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_15a4ce14f3334916bdba1aa35701b210 = L.circleMarker(
                [25.14389, 75.82125599999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_b365b74653cf48729dd70261db67de73 = L.circleMarker(
                [28.5504249, 77.21593770000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_3dba509600a24b678eff7e117009cf8a = L.circleMarker(
                [26.83399722000001, 80.89173609999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_ac11fa0c93994e1685dacc32aece0df7 = L.circleMarker(
                [13.669999999999996, 79.35000000000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_4db995951ae14d519ef3f56e56049312 = L.circleMarker(
                [13.087840000000005, 80.27846999999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_8b81f62cfada4a09ad805e064ccbbd2d = L.circleMarker(
                [17.38405, 78.45635999999996],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_31b72e018e7e41b08517b58821bdd611 = L.circleMarker(
                [22.56263000000001, 88.36303999999997],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_7d805db582474a5da36bcaaaf3ca7d69 = L.circleMarker(
                [19.072830000000007, 72.88261000000004],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_d71c932e742b416b9319e3986d1a05c8 = L.circleMarker(
                [28.63576, 77.22444999999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_422f92cb5756457283fe4e312c965431 = L.circleMarker(
                [28.660334600000002, 77.3572563],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_ac6e65ef03da4a9199d21319165b6c53 = L.circleMarker(
                [28.45012380000001, 77.02630509999999],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_20d2518deae54cd48bf4520407205140 = L.circleMarker(
                [24.108970000000003, 82.64558000000001],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_d5693a9b21804bc9ba07a7a97c3d5f10 = L.circleMarker(
                [26.688304899999995, 88.41266800000004],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
            var circle_marker_b16413e193914cd9b2010cb8c35ddd43 = L.circleMarker(
                [17.349694, 78.45143699999998],
                {"bubblingMouseEvents": true, "color": "#000000", "dashArray": null, "dashOffset": null, "fill": true, "fillColor": "#D3D3D3", "fillOpacity": 1, "fillRule": "evenodd", "lineCap": "round", "lineJoin": "round", "opacity": 1.0, "radius": 5, "stroke": true, "weight": 3}
            ).addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        
    
        function geo_json_3617fe072f6f48afaa08d8abaf106855_onEachFeature(feature, layer) {
            layer.on({
                click: function(e) {
                    map_60c2bfe17360488ebe908eecf9e486f8.fitBounds(e.target.getBounds());
                }
            });
        };
        var geo_json_3617fe072f6f48afaa08d8abaf106855 = L.geoJson(null, {
                onEachFeature: geo_json_3617fe072f6f48afaa08d8abaf106855_onEachFeature,
            
        });
        function geo_json_3617fe072f6f48afaa08d8abaf106855_add (data) {
            geo_json_3617fe072f6f48afaa08d8abaf106855.addData(data)
                .addTo(map_60c2bfe17360488ebe908eecf9e486f8);
        }
            geo_json_3617fe072f6f48afaa08d8abaf106855_add({"geometry": {"coordinates": [[[77.837451, 35.49401], [78.912269, 34.321936], [78.811086, 33.506198], [79.208892, 32.994395], [79.176129, 32.48378], [78.458446, 32.618164], [78.738894, 31.515906], [79.721367, 30.882715], [81.111256, 30.183481], [80.476721, 29.729865], [80.088425, 28.79447], [81.057203, 28.416095], [81.999987, 27.925479], [83.304249, 27.364506], [84.675018, 27.234901], [85.251779, 26.726198], [86.024393, 26.630985], [87.227472, 26.397898], [88.060238, 26.414615], [88.174804, 26.810405], [88.043133, 27.445819], [88.120441, 27.876542], [88.730326, 28.086865], [88.814248, 27.299316], [88.835643, 27.098966], [89.744528, 26.719403], [90.373275, 26.875724], [91.217513, 26.808648], [92.033484, 26.83831], [92.103712, 27.452614], [91.696657, 27.771742], [92.503119, 27.896876], [93.413348, 28.640629], [94.56599, 29.277438], [95.404802, 29.031717], [96.117679, 29.452802], [96.586591, 28.83098], [96.248833, 28.411031], [97.327114, 28.261583], [97.402561, 27.882536], [97.051989, 27.699059], [97.133999, 27.083774], [96.419366, 27.264589], [95.124768, 26.573572], [95.155153, 26.001307], [94.603249, 25.162495], [94.552658, 24.675238], [94.106742, 23.850741], [93.325188, 24.078556], [93.286327, 23.043658], [93.060294, 22.703111], [93.166128, 22.27846], [92.672721, 22.041239], [92.146035, 23.627499], [91.869928, 23.624346], [91.706475, 22.985264], [91.158963, 23.503527], [91.46773, 24.072639], [91.915093, 24.130414], [92.376202, 24.976693], [91.799596, 25.147432], [90.872211, 25.132601], [89.920693, 25.26975], [89.832481, 25.965082], [89.355094, 26.014407], [88.563049, 26.446526], [88.209789, 25.768066], [88.931554, 25.238692], [88.306373, 24.866079], [88.084422, 24.501657], [88.69994, 24.233715], [88.52977, 23.631142], [88.876312, 22.879146], [89.031961, 22.055708], [88.888766, 21.690588], [88.208497, 21.703172], [86.975704, 21.495562], [87.033169, 20.743308], [86.499351, 20.151638], [85.060266, 19.478579], [83.941006, 18.30201], [83.189217, 17.671221], [82.192792, 17.016636], [82.191242, 16.556664], [81.692719, 16.310219], [80.791999, 15.951972], [80.324896, 15.899185], [80.025069, 15.136415], [80.233274, 13.835771], [80.286294, 13.006261], [79.862547, 12.056215], [79.857999, 10.357275], [79.340512, 10.308854], [78.885345, 9.546136], [79.18972, 9.216544], [78.277941, 8.933047], [77.941165, 8.252959], [77.539898, 7.965535], [76.592979, 8.899276], [76.130061, 10.29963], [75.746467, 11.308251], [75.396101, 11.781245], [74.864816, 12.741936], [74.616717, 13.992583], [74.443859, 14.617222], [73.534199, 15.990652], [73.119909, 17.92857], [72.820909, 19.208234], [72.824475, 20.419503], [72.630533, 21.356009], [71.175273, 20.757441], [70.470459, 20.877331], [69.16413, 22.089298], [69.644928, 22.450775], [69.349597, 22.84318], [68.176645, 23.691965], [68.842599, 24.359134], [71.04324, 24.356524], [70.844699, 25.215102], [70.282873, 25.722229], [70.168927, 26.491872], [69.514393, 26.940966], [70.616496, 27.989196], [71.777666, 27.91318], [72.823752, 28.961592], [73.450638, 29.976413], [74.42138, 30.979815], [74.405929, 31.692639], [75.258642, 32.271105], [74.451559, 32.7649], [74.104294, 33.441473], [73.749948, 34.317699], [74.240203, 34.748887], [75.757061, 34.504923], [76.871722, 34.653544], [77.837451, 35.49401]]], "type": "Polygon"}, "id": "IND", "properties": {"name": "India"}, "type": "Feature"});
        
</script> onload= this. contentDocument. open();this. contentDocument. write(atob(this. getAttribute('data-html')));this. contentDocument. close();  allowfullscreen webkitallowfullscreen mozallowfullscreen&gt;&lt;/iframe&gt;  There you go! "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')
    this.metadataWhitelist = ['position']

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}